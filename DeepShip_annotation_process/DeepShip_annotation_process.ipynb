{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6fe861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re, csv\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54633973",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"ID\", \"class ID\", \"Recording ID\", \"Ship Name\",\n",
    "           \"Date & Time\", \"Duration(sec)\", \"Distances(m)\"]\n",
    "           \n",
    "def clean_line(s):\n",
    "    # 统一空白和连字符\n",
    "    return (s.replace(\"\\u00A0\", \" \")   # nbsp -> space\n",
    "             .replace(\"\\u2013\", \"-\")  # en-dash –\n",
    "             .replace(\"\\u2014\", \"-\")  # em-dash —\n",
    "             .replace(\"\\u2212\", \"-\")  # minus sign −\n",
    "             .strip())\n",
    "\n",
    "def parse_file(path):\n",
    "    rows, bad = [], 0\n",
    "    with open(path, \"r\") as f:\n",
    "        for line_num, raw in enumerate(f, 1):\n",
    "            line = clean_line(raw)\n",
    "\n",
    "            # 跳过空行\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # 使用逗号分割数据\n",
    "            parts = [part.strip() for part in line.split(',')]\n",
    "            \n",
    "            # 提取各个字段\n",
    "            id = parts[0].strip()\n",
    "            class_id = parts[1].strip() \n",
    "            ship_name = parts[2].strip()\n",
    "            date_part = parts[3].strip()\n",
    "            time_part = parts[4].strip()\n",
    "            duration = parts[5].strip()\n",
    "            distances = parts[6].strip()\n",
    "            \n",
    "            # 合并日期和时间，格式化为 YYYYMMDD:HHMMSS\n",
    "            date_time = f\"{date_part}:{time_part}\"\n",
    "            \n",
    "            # 这里假设 Recording ID 和 ID 相同，或者可以根据需要调整\n",
    "            recording_id = id # 或者可以设置为其他值\n",
    "            \n",
    "            rows.append([id, class_id, recording_id, ship_name, date_time, duration, distances])\n",
    "    \n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3d0bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: E:\\数据集\\DeepShip\\data_preprocessing\\annotation\\tug-metafile.csv)\n"
     ]
    }
   ],
   "source": [
    "in_path = Path(r\"E:\\数据集\\DeepShip\\data_preprocessing\\annotation\\tug-metafile\")\n",
    "out_path = in_path.with_suffix(\".csv\")\n",
    "rows = parse_file(in_path)\n",
    "with open(out_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(COLUMNS)\n",
    "        writer.writerows(rows)\n",
    "print(f\"Saved: {out_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9fe37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_original_dir = Path(r\"E:\\数据集\\DeepShip\\data_preprocessing\\annotation_original\")  # 换成实际放 CSV 的目录\n",
    "\n",
    "data_dir = annotation_original_dir\n",
    "files = {\n",
    "    \"cargo\": data_dir / \"cargo-metafile.csv\",\n",
    "    \"passenger\": data_dir / \"passengership-metafile.csv\",\n",
    "    \"tanker\": data_dir / \"tanker-metafile.csv\",\n",
    "    \"tug\": data_dir / \"tug-metafile.csv\",\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "for label, path in files.items():\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "merged = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "merged.to_csv(r\"E:\\数据集\\DeepShip\\data_preprocessing\\annotation\\DeepShip.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7874eab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargo\n",
      "  annotated_count: 110\n",
      "  folder_count: 109\n",
      "  missing_ids: [23]\n",
      "  extra_ids: OK\n",
      "  invalid_folder_names: OK\n",
      "  duplicate_folder_names: OK\n",
      "  missing_wav_files: OK\n",
      "  renamed_wav_files: OK\n",
      "----------------------------------------\n",
      "Passengership\n",
      "  annotated_count: 191\n",
      "  folder_count: 191\n",
      "  missing_ids: OK\n",
      "  extra_ids: OK\n",
      "  invalid_folder_names: OK\n",
      "  duplicate_folder_names: OK\n",
      "  missing_wav_files: OK\n",
      "  renamed_wav_files: OK\n",
      "----------------------------------------\n",
      "Tanker\n",
      "  annotated_count: 240\n",
      "  folder_count: 240\n",
      "  missing_ids: OK\n",
      "  extra_ids: OK\n",
      "  invalid_folder_names: OK\n",
      "  duplicate_folder_names: OK\n",
      "  missing_wav_files: OK\n",
      "  renamed_wav_files: OK\n",
      "----------------------------------------\n",
      "Tug\n",
      "  annotated_count: 69\n",
      "  folder_count: 69\n",
      "  missing_ids: OK\n",
      "  extra_ids: OK\n",
      "  invalid_folder_names: OK\n",
      "  duplicate_folder_names: OK\n",
      "  missing_wav_files: OK\n",
      "  renamed_wav_files: OK\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data_root = Path(r\"E:\\数据集\\DeepShip\\data_preprocessing\\data\")\n",
    "anno_root = Path(r\"E:\\数据集\\DeepShip\\data_preprocessing\\annotation_original\")\n",
    "\n",
    "meta_files = {\n",
    "    \"Cargo\": anno_root / \"cargo-metafile.csv\",\n",
    "    \"Passengership\": anno_root / \"passengership-metafile.csv\",\n",
    "    \"Tanker\": anno_root / \"tanker-metafile.csv\",\n",
    "    \"Tug\": anno_root / \"tug-metafile.csv\",\n",
    "}\n",
    "\n",
    "def parse_id(folder_name: str) -> int | None:\n",
    "    parts = folder_name.rsplit(\"-\", 1)\n",
    "    if len(parts) != 2 or not parts[1].isdigit():\n",
    "        return None\n",
    "    return int(parts[1])\n",
    "\n",
    "summary = {}\n",
    "\n",
    "for cls, meta_path in meta_files.items():\n",
    "    ann = pd.read_csv(meta_path)\n",
    "    expected_ids = set(ann[\"ID\"].astype(int))\n",
    "\n",
    "    cls_dir = data_root / cls\n",
    "    if not cls_dir.exists():\n",
    "        summary[cls] = {\"error\": f\"目录不存在: {cls_dir}\"}\n",
    "        continue\n",
    "\n",
    "    folder_ids: dict[int, Path] = {}\n",
    "    invalid_names: list[str] = []\n",
    "    duplicate_names: list[str] = []\n",
    "\n",
    "    for subdir in cls_dir.iterdir():\n",
    "        if not subdir.is_dir():\n",
    "            continue\n",
    "        sid = parse_id(subdir.name)\n",
    "        if sid is None:\n",
    "            invalid_names.append(subdir.name)\n",
    "            continue\n",
    "        # 如果同一个 ID 出现多个文件夹，记录重复名称\n",
    "        if sid in folder_ids:\n",
    "            duplicate_names.append(subdir.name)\n",
    "        else:\n",
    "            folder_ids[sid] = subdir\n",
    "\n",
    "    observed_ids = set(folder_ids)\n",
    "    missing_ids = sorted(expected_ids - observed_ids)\n",
    "    extra_ids = sorted(observed_ids - expected_ids)\n",
    "\n",
    "    missing_audio = []\n",
    "    renamed_audio = []\n",
    "\n",
    "    for sid, folder in folder_ids.items():\n",
    "        expected_wav = folder / f\"{sid}.wav\"\n",
    "        if expected_wav.exists():\n",
    "            # 名称已经对上\n",
    "            continue\n",
    "\n",
    "        # 这里没有标准命名，尝试重命名\n",
    "        wav_files = list(folder.glob(\"*.wav\"))\n",
    "\n",
    "        if len(wav_files) == 1:\n",
    "            src = wav_files[0]\n",
    "            src.rename(expected_wav)\n",
    "            renamed_audio.append(f\"{src.name} -> {expected_wav.name}\")\n",
    "        elif len(wav_files) > 1:\n",
    "            missing_audio.append(\n",
    "                f\"{folder}（发现多个 wav：{', '.join(f.name for f in wav_files)}）\"\n",
    "            )\n",
    "        else:\n",
    "            missing_audio.append(f\"{expected_wav}（文件夹没有 wav）\")\n",
    "\n",
    "    summary[cls] = {\n",
    "        \"annotated_count\": len(expected_ids),\n",
    "        \"folder_count\": len(observed_ids),\n",
    "        \"missing_ids\": missing_ids,\n",
    "        \"extra_ids\": extra_ids,\n",
    "        \"invalid_folder_names\": invalid_names,\n",
    "        \"duplicate_folder_names\": duplicate_names,\n",
    "        \"missing_wav_files\": missing_audio,\n",
    "        \"renamed_wav_files\": renamed_audio if renamed_audio else \"OK\",\n",
    "    }\n",
    "\n",
    "for cls, info in summary.items():\n",
    "    print(cls)\n",
    "    for key, value in info.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"  {key}: {value if value else 'OK'}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f89e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
