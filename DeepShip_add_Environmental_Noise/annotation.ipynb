{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c81520a2",
   "metadata": {},
   "source": [
    "# DeepShip æ•°æ®é›†åŠ ç¯å¢ƒå™ªå£°ç±»åçš„æ ‡æ³¨æ–‡ä»¶å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253b2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f565bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸè¯»å– CSVæ–‡ä»¶ï¼Œå…± 610 æ¡è®°å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "# é…ç½®è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "csv_path = r'X:\\\\æ•°æ®é›†\\\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip.csv'\n",
    "train_txt_path = r'X:\\\\æ•°æ®é›†\\\\DeepShip\\\\annotation_original\\\\training_and_testing\\\\train.txt'\n",
    "test_txt_path = r'X:\\\\æ•°æ®é›†\\\\DeepShip\\\\annotation_original\\\\training_and_testing\\\\test.txt'\n",
    "output_csv_path = r'X:\\\\æ•°æ®é›†\\\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip_No_Overlap_Metadata.csv'\n",
    "\n",
    "# è¯»å–åŸå§‹ CSV\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"æˆåŠŸè¯»å– CSVæ–‡ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•ã€‚\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ–‡ä»¶: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96758a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–°æ–‡ä»¶åç¤ºä¾‹:\n",
      "   ID  class_id new_filename\n",
      "0   1         0      0_1.wav\n",
      "1   2         0      0_2.wav\n",
      "2   3         0      0_3.wav\n",
      "3   4         0      0_4.wav\n",
      "4   5         0      0_5.wav\n"
     ]
    }
   ],
   "source": [
    "df['new_filename'] = df.apply(lambda row: f\"{row['class_id']}_{row['ID']}.wav\", axis=1)\n",
    "\n",
    "print(\"æ–°æ–‡ä»¶åç¤ºä¾‹:\")\n",
    "print(df[['ID', 'class_id', 'new_filename']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42133b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_keys(file_path):\n",
    "    keys = set()\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}\")\n",
    "        return keys\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            parts = line.split('/')\n",
    "            if len(parts) >= 3:\n",
    "                class_name = parts[-3]\n",
    "                folder_name = parts[-2]\n",
    "                keys.add((class_name, folder_name))\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407996e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä» X:\\\\æ•°æ®é›†\\\\DeepShip\\\\annotation_original\\\\training_and_testing\\\\train.txt è§£æå‡º 398 ä¸ªå”¯ä¸€æ–‡ä»¶å¤¹ (Train)\n",
      "ä» X:\\\\æ•°æ®é›†\\\\DeepShip\\\\annotation_original\\\\training_and_testing\\\\test.txt è§£æå‡º 211 ä¸ªå”¯ä¸€æ–‡ä»¶å¤¹ (Test)\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ‡è¯†é”®\n",
    "train_keys = get_split_keys(train_txt_path)\n",
    "test_keys = get_split_keys(test_txt_path)\n",
    "\n",
    "print(f\"ä» {train_txt_path} è§£æå‡º {len(train_keys)} ä¸ªå”¯ä¸€æ–‡ä»¶å¤¹ (Train)\")\n",
    "print(f\"ä» {test_txt_path} è§£æå‡º {len(test_keys)} ä¸ªå”¯ä¸€æ–‡ä»¶å¤¹ (Test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbd5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡:\n",
      "dataset_split\n",
      "train         398\n",
      "test          211\n",
      "unassigned      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "æ³¨æ„ï¼šä»¥ä¸‹æ•°æ®æœªè¢«åˆ†é… (å¯èƒ½ç¼ºå¤±å…ƒæ•°æ®):\n",
      "    ID Ship Name folder_name\n",
      "21  23   GALLEON         NaN\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {\n",
    "    0: 'Cargo',\n",
    "    1: 'Passengership',\n",
    "    2: 'Tanker',\n",
    "    3: 'Tug'\n",
    "}\n",
    "\n",
    "def assign_split_label(row):\n",
    "    if pd.isna(row['folder_name']): return 'unassigned'\n",
    "    txt_class = class_mapping.get(row['class_id'])\n",
    "    key = (txt_class, row['folder_name'])\n",
    "    \n",
    "    if key in train_keys: return 'train'\n",
    "    elif key in test_keys: return 'test'\n",
    "    else: return 'unassigned'\n",
    "\n",
    "df['dataset_split'] = df.apply(assign_split_label, axis=1)\n",
    "\n",
    "# æ‰“å°ç»Ÿè®¡ç»“æœ\n",
    "split_counts = df['dataset_split'].value_counts()\n",
    "print(\"\\næ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡:\")\n",
    "print(split_counts)\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰æœªåˆ†é…çš„æ•°æ® (é€šå¸¸æ˜¯ID 23, å› ä¸ºç¼ºå¤±folder_name)\n",
    "unassigned = df[df['dataset_split'] == 'unassigned']\n",
    "if len(unassigned) > 0:\n",
    "    print(\"\\næ³¨æ„ï¼šä»¥ä¸‹æ•°æ®æœªè¢«åˆ†é… (å¯èƒ½ç¼ºå¤±å…ƒæ•°æ®):\")\n",
    "    print(unassigned[['ID', 'Ship Name', 'folder_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9e00a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²å‰”é™¤æœªåˆ†é…æ•°æ® 1 æ¡ã€‚\n",
      "\n",
      "æœ€ç»ˆåˆ’åˆ†ç»Ÿè®¡ç»“æœ:\n",
      "dataset_split\n",
      "train    398\n",
      "test     211\n",
      "Name: count, dtype: int64\n",
      "\n",
      "å¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜ä¸º: X:\\\\æ•°æ®é›†\\\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip_No_Overlap_Metadata.csv\n"
     ]
    }
   ],
   "source": [
    "df_clean = df[df['dataset_split'] != 'unassigned'].copy()\n",
    "print(f\"å·²å‰”é™¤æœªåˆ†é…æ•°æ® {len(df) - len(df_clean)} æ¡ã€‚\")\n",
    "\n",
    "target_columns = [\n",
    "    'ID', \n",
    "    'new_filename',\n",
    "    'class_id', \n",
    "    'class ID', \n",
    "    'Ship Name', \n",
    "    'folder_name', \n",
    "    'Date & Time',\n",
    "    'Duration(sec)',\n",
    "    'Distances(m)',\n",
    "    'prompt_en',\n",
    "    'dataset_split'\n",
    "]\n",
    "\n",
    "# ç¡®ä¿åˆ—éƒ½å­˜åœ¨\n",
    "df_final = df_clean[target_columns]\n",
    "\n",
    "# æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"\\næœ€ç»ˆåˆ’åˆ†ç»Ÿè®¡ç»“æœ:\")\n",
    "print(df_final['dataset_split'].value_counts())\n",
    "\n",
    "# ä¿å­˜æ–‡ä»¶\n",
    "df_final.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nå¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜ä¸º: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b912a7",
   "metadata": {},
   "source": [
    "# å¤„ç†æµ·æ´‹ç¯å¢ƒå™ªå£°çš„æ ‡æ³¨ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d63dbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_csv_path = r\"X:\\\\æ•°æ®é›†\\\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip_No_Overlap_Metadata.csv\"\n",
    "output_csv_path = r\"X:\\æ•°æ®é›†\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip_No_Overlap_Metadata_add_Environmental_Noise.csv\"\n",
    "\n",
    "# å™ªå£°æ•°æ®çš„æ ¹ç›®å½•\n",
    "NOISE_ROOT_PATH = r'X:\\æ•°æ®é›†\\DeepShip\\data_original\\background'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4033e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹ 4 ç±»æ•°æ®åŠ è½½å®Œæˆ: 609 æ¡è®°å½•\n"
     ]
    }
   ],
   "source": [
    "df_ship = pd.read_csv(original_csv_path)\n",
    "print(f\"åŸå§‹ 4 ç±»æ•°æ®åŠ è½½å®Œæˆ: {len(df_ship)} æ¡è®°å½•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468dce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å¼€å§‹æ‰§è¡Œæ–‡ä»¶é‡å‘½å ===\n",
      "æ­£åœ¨æ‰«æç›®å½•: X:\\æ•°æ®é›†\\DeepShip\\data_original\\background\\train\n",
      "æ­£åœ¨æ‰«æç›®å½•: X:\\æ•°æ®é›†\\DeepShip\\data_original\\background\\test\n",
      "å·²é‡å‘½å 27700 ä¸ªæ–‡ä»¶...\n",
      "é‡å‘½åå®Œæˆ! å…±ä¿®æ”¹ 27768 ä¸ªæ–‡ä»¶, å¤±è´¥ 0 ä¸ªã€‚\n"
     ]
    }
   ],
   "source": [
    "print(\"=== å¼€å§‹æ‰§è¡Œæ–‡ä»¶é‡å‘½å ===\")\n",
    "def rename_noise_files(root_path):\n",
    "    count = 0\n",
    "    errors = 0\n",
    "    \n",
    "    # éå† train å’Œ test\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = os.path.join(root_path, split)\n",
    "        if not os.path.exists(split_dir):\n",
    "            print(f\"è­¦å‘Š: æ‰¾ä¸åˆ°ç›®å½• {split_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"æ­£åœ¨æ‰«æç›®å½•: {split_dir}\")\n",
    "        \n",
    "        # ä½¿ç”¨ os.walk éå†\n",
    "        for root, dirs, files in os.walk(split_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav'):\n",
    "                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å·²ç»æ˜¯ç›®æ ‡æ ¼å¼ (é˜²æ­¢é‡å¤è¿è¡Œå¯¼è‡´é”™è¯¯)\n",
    "                    if file.startswith('4_') and file.count('_') >= 2:\n",
    "                        continue\n",
    "                        \n",
    "                    old_path = os.path.join(root, file)\n",
    "                    \n",
    "                    try:\n",
    "                        # === è§£æè·¯å¾„é€»è¾‘ ===\n",
    "                        # ç¤ºä¾‹è·¯å¾„: ...\\3_0\\0000\\00000.wav\n",
    "                        path_parts = os.path.normpath(old_path).split(os.sep)\n",
    "                        \n",
    "                        # 1. æå– FileID (00000 -> 1)\n",
    "                        file_stem = os.path.splitext(file)[0]\n",
    "                        try:\n",
    "                            file_num_id = int(file_stem) + 1 # 00000 -> 1\n",
    "                        except ValueError:\n",
    "                            # å¦‚æœæ–‡ä»¶åä¸æ˜¯çº¯æ•°å­—ï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œæˆ–è€…è·³è¿‡\n",
    "                            print(f\"è·³è¿‡éæ•°å­—æ–‡ä»¶å: {file}\")\n",
    "                            continue\n",
    "\n",
    "                        # 2. æå– FolderID (3_0 -> 0)\n",
    "                        # å¯»æ‰¾åŒ…å« '_' çš„ä¸Šçº§ç›®å½•\n",
    "                        parent = path_parts[-2]      # 0000\n",
    "                        grandparent = path_parts[-3] # 3_0\n",
    "                        \n",
    "                        target_folder = grandparent if '_' in grandparent else parent\n",
    "                        \n",
    "                        if '_' in target_folder:\n",
    "                            folder_id_part = target_folder.split('_')[-1]\n",
    "                        else:\n",
    "                            # å¦‚æœæ‰¾ä¸åˆ°ä¸‹åˆ’çº¿ï¼Œå°è¯•ç”¨å½“å‰ç›®å½•å\n",
    "                            folder_id_part = target_folder\n",
    "                            \n",
    "                        # === æ„é€ æ–°æ–‡ä»¶å ===\n",
    "                        # æ ¼å¼: 4_{FolderID}_{FileID}.wav\n",
    "                        new_filename = f\"4_{folder_id_part}_{file_num_id}.wav\"\n",
    "                        new_path = os.path.join(root, new_filename)\n",
    "                        \n",
    "                        # === æ‰§è¡Œé‡å‘½å ===\n",
    "                        if old_path != new_path:\n",
    "                            os.rename(old_path, new_path)\n",
    "                            count += 1\n",
    "                            if count % 100 == 0:\n",
    "                                print(f\"å·²é‡å‘½å {count} ä¸ªæ–‡ä»¶...\", end='\\r')\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"é‡å‘½åå¤±è´¥: {old_path}, é”™è¯¯: {e}\")\n",
    "                        errors += 1\n",
    "\n",
    "    print(f\"\\né‡å‘½åå®Œæˆ! å…±ä¿®æ”¹ {count} ä¸ªæ–‡ä»¶, å¤±è´¥ {errors} ä¸ªã€‚\")\n",
    "    \n",
    "# æ‰§è¡Œé‡å‘½åå‡½æ•°\n",
    "rename_noise_files(NOISE_ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cea207f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== å¼€å§‹ç”Ÿæˆæ ‡æ³¨ä¿¡æ¯ ===\n",
      "æˆåŠŸè¯»å–åŸå§‹æ•°æ®ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== å¼€å§‹ç”Ÿæˆæ ‡æ³¨ä¿¡æ¯ ===\")\n",
    "\n",
    "if os.path.exists(original_csv_path):\n",
    "    df_ship = pd.read_csv(original_csv_path)\n",
    "    print(\"æˆåŠŸè¯»å–åŸå§‹æ•°æ®ã€‚\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"æœªæ‰¾åˆ°åŸå§‹æ–‡ä»¶: {original_csv_path}\")\n",
    "\n",
    "# æ‰«æå·²é‡å‘½åçš„æ–‡ä»¶\n",
    "def scan_renamed_files(root_path):\n",
    "    noise_rows = []\n",
    "    # === ä¿®æ”¹å¤„ï¼šID ä» 1 å¼€å§‹ç‹¬ç«‹è®¡æ•° ===\n",
    "    current_id = 1 \n",
    "    \n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = os.path.join(root_path, split)\n",
    "        if not os.path.exists(split_dir): continue\n",
    "        \n",
    "        for root, dirs, files in os.walk(split_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.wav') and file.startswith('4_'):\n",
    "                    \n",
    "                    # è·å– folder_name\n",
    "                    path_parts = os.path.normpath(root).split(os.sep)\n",
    "                    parent = path_parts[-1] \n",
    "                    grandparent = path_parts[-2] \n",
    "                    raw_folder_name = grandparent if '_' in grandparent else parent\n",
    "\n",
    "                    row = {\n",
    "                        'ID': current_id,\n",
    "                        'new_filename': file,  \n",
    "                        'class_id': 4,\n",
    "                        'class ID': None,\n",
    "                        'Ship Name': 'Marine Environmental Noise',\n",
    "                        'folder_name': raw_folder_name, \n",
    "                        'Date & Time': 'Unknown',\n",
    "                        'Duration(sec)': 3,\n",
    "                        'Distances(m)': None,\n",
    "                        'prompt_en': 'Hydrophone recording of marine environmental noise.',\n",
    "                        'dataset_split': split\n",
    "                    }\n",
    "                    noise_rows.append(row)\n",
    "                    # è®¡æ•°å™¨é€’å¢\n",
    "                    current_id += 1\n",
    "                    \n",
    "    return pd.DataFrame(noise_rows)\n",
    "\n",
    "# æ‰§è¡Œæ‰«æ\n",
    "df_noise = scan_renamed_files(NOISE_ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e2931cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰«æåˆ° 27768 æ¡å™ªå£°æ•°æ®ã€‚\n",
      "å™ªå£°æ•°æ®çš„ ID èŒƒå›´: 1 - 27768\n",
      "\n",
      "æœ€ç»ˆå¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: X:\\æ•°æ®é›†\\DeepShip\\\\data_preprocessing\\\\annotation\\\\DeepShip_No_Overlap_Metadata_add_Environmental_Noise.csv\n"
     ]
    }
   ],
   "source": [
    "if len(df_noise) > 0:\n",
    "    print(f\"æ‰«æåˆ° {len(df_noise)} æ¡å™ªå£°æ•°æ®ã€‚\")\n",
    "    print(f\"å™ªå£°æ•°æ®çš„ ID èŒƒå›´: {df_noise['ID'].min()} - {df_noise['ID'].max()}\")\n",
    "    \n",
    "    # 2.3 åˆå¹¶æ•°æ®\n",
    "    target_columns = [\n",
    "        'ID', 'new_filename', 'class_id', 'class ID', \n",
    "        'Ship Name', 'folder_name', 'Date & Time', \n",
    "        'Duration(sec)', 'Distances(m)', 'prompt_en', 'dataset_split'\n",
    "    ]\n",
    "    \n",
    "    # ç¡®ä¿åˆ—ä¸€è‡´\n",
    "    df_ship_aligned = df_ship[target_columns]\n",
    "    df_noise_aligned = df_noise[target_columns]\n",
    "    \n",
    "    # åˆå¹¶\n",
    "    df_final = pd.concat([df_ship_aligned, df_noise_aligned], ignore_index=True)\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    df_final.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\næœ€ç»ˆå¤„ç†å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜è‡³: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"æœªæ‰¾åˆ°ä»»ä½•å™ªå£°æ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33449b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92d1d3ff",
   "metadata": {},
   "source": [
    "# æ ¹æ®åˆ‡å‰²å¥½çš„éŸ³é¢‘é‡æ–°å¤„ç†æ ‡æ³¨ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f41783",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_CSV_PATH = r\"X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\annotation\\DeepShip_No_Overlap_Metadata_add_Environmental_Noise.csv\"\n",
    "\n",
    "PREPROCESSED_ROOT = Path(r\"X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\data_audio_rename_add_enviromental_noise_class_dataset_split_preprocessed\")\n",
    "\n",
    "OUTPUT_CSV_PATH = r\"X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\annotation\\DeepShip_No_Overlap_add_Environmental_Noise_Segmented_Metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac733fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_lookup(csv_path):\n",
    "    \"\"\"\n",
    "    åŠ è½½åŸå§‹ CSV å¹¶å»ºç«‹æŸ¥æ‰¾è¡¨\n",
    "    Key: æ–‡ä»¶å stem (ä¸å«åç¼€), ä¾‹å¦‚ \"0_3\" æˆ– \"4_0_1\"\n",
    "    Value: è¯¥è¡Œçš„ Series æ•°æ®\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"æ‰¾ä¸åˆ°åŸå§‹æ ‡æ³¨æ–‡ä»¶: {csv_path}\")\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    lookup = {}\n",
    "    \n",
    "    # å»ºç«‹ filename_stem -> row çš„æ˜ å°„\n",
    "    for idx, row in df.iterrows():\n",
    "        # è·å– new_filename çš„ stem (ä¾‹å¦‚ \"0_3.wav\" -> \"0_3\")\n",
    "        fname = str(row['new_filename'])\n",
    "        if fname.lower().endswith('.wav'):\n",
    "            stem = fname[:-4]\n",
    "        else:\n",
    "            stem = fname\n",
    "        lookup[stem] = row\n",
    "    \n",
    "    print(f\"åŸå§‹å…ƒæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(lookup)} æ¡ç´¢å¼•ã€‚\")\n",
    "    return lookup\n",
    "\n",
    "def generate_segmented_metadata():\n",
    "    # 1. åŠ è½½æŸ¥æ‰¾è¡¨\n",
    "    try:\n",
    "        meta_lookup = load_metadata_lookup(ORIGINAL_CSV_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é”™è¯¯: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. å‡†å¤‡æ–°æ•°æ®åˆ—è¡¨\n",
    "    new_rows = []\n",
    "    \n",
    "    # 3. éå†é¢„å¤„ç†åçš„æ–‡ä»¶å¤¹\n",
    "    # æœŸæœ›ç»“æ„: root / split / category / file.wav\n",
    "    if not PREPROCESSED_ROOT.exists():\n",
    "        print(f\"âŒ é”™è¯¯: é¢„å¤„ç†ç›®å½•ä¸å­˜åœ¨ {PREPROCESSED_ROOT}\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰«æå¹¶ç”Ÿæˆæ–°æ ‡æ³¨...\")\n",
    "    \n",
    "    # å…¨å±€ ID è®¡æ•°å™¨ (æ–°æ–‡ä»¶ä» 1 å¼€å§‹ç¼–å·)\n",
    "    global_id_counter = 1\n",
    "    \n",
    "    # éå† train å’Œ test\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = PREPROCESSED_ROOT / split\n",
    "        if not split_dir.exists(): continue\n",
    "        \n",
    "        # éå†ç±»åˆ« (Cargo, Background, ...)\n",
    "        for category_dir in split_dir.iterdir():\n",
    "            if not category_dir.is_dir(): continue\n",
    "            \n",
    "            category_name = category_dir.name\n",
    "            \n",
    "            # åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°ç±» (Background)\n",
    "            # å¦‚æœæ˜¯å™ªå£°ç±»ï¼Œæ–‡ä»¶åæ²¡æœ‰å˜ (4_0_1.wav -> 4_0_1)\n",
    "            # å¦‚æœæ˜¯èˆ¹åªç±»ï¼Œæ–‡ä»¶åå˜äº† (0_3.wav -> 0_3_1)\n",
    "            is_noise = (category_name == \"Background\")\n",
    "            \n",
    "            files = sorted(list(category_dir.glob(\"*.wav\")))\n",
    "            \n",
    "            for wav_file in tqdm(files, desc=f\"Scanning {split}/{category_name}\", leave=False):\n",
    "                current_stem = wav_file.stem  # ä¾‹å¦‚ \"0_3_1\" æˆ– \"4_0_1\"\n",
    "                \n",
    "                # === A. ç¡®å®šçˆ¶æ–‡ä»¶çš„ Key ===\n",
    "                parent_key = None\n",
    "                \n",
    "                if is_noise:\n",
    "                    # å™ªå£°ç±»æ–‡ä»¶åæœªå˜ï¼Œç›´æ¥æŸ¥\n",
    "                    parent_key = current_stem\n",
    "                else:\n",
    "                    # èˆ¹åªç±»æ–‡ä»¶åå¤šäº†åç¼€ \"_1\", \"_2\"\n",
    "                    # è§£æé€»è¾‘: 0_3_1 -> 0_3\n",
    "                    # ä½¿ç”¨ rsplit åˆ‡åˆ†æœ€åä¸€ä¸ªä¸‹åˆ’çº¿\n",
    "                    if '_' in current_stem:\n",
    "                        parent_key = current_stem.rsplit('_', 1)[0]\n",
    "                    else:\n",
    "                        # å¼‚å¸¸æƒ…å†µï¼Œé˜²æŠ¥é”™\n",
    "                        parent_key = current_stem\n",
    "                \n",
    "                # === B. æŸ¥æ‰¾å…ƒæ•°æ® ===\n",
    "                parent_info = meta_lookup.get(parent_key)\n",
    "                \n",
    "                if parent_info is None:\n",
    "                    # å¦‚æœæŸ¥ä¸åˆ°ï¼Œæ‰“å°è­¦å‘Š (å¯èƒ½æœ‰äº›æ–‡ä»¶è¢«åˆ äº†æˆ–è€…å‘½åå¯¹ä¸ä¸Š)\n",
    "                    # print(f\"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°çˆ¶æ ·æœ¬ä¿¡æ¯: {parent_key} (å½“å‰æ–‡ä»¶: {wav_file.name})\")\n",
    "                    continue\n",
    "                \n",
    "                # === C. æ„å»ºæ–°è¡Œ ===\n",
    "                new_row = parent_info.copy()\n",
    "                \n",
    "                # æ›´æ–°å…³é”®å­—æ®µ\n",
    "                new_row['ID'] = global_id_counter\n",
    "                new_row['new_filename'] = wav_file.name # æ›´æ–°ä¸ºåˆ‡ç‰‡åçš„æ–‡ä»¶å\n",
    "                new_row['Duration(sec)'] = 3.0          # ç»Ÿä¸€ä¿®æ­£ä¸º 3ç§’\n",
    "                new_row['dataset_split'] = split        # ä»¥å®é™…æ‰€åœ¨æ–‡ä»¶å¤¹ä¸ºå‡†\n",
    "                \n",
    "                # å¯ä»¥é€‰æ‹©ä¿ç•™ parent_filename ä»¥ä¾¿è¿½æº¯\n",
    "                new_row['parent_filename'] = parent_info['new_filename']\n",
    "                \n",
    "                new_rows.append(new_row)\n",
    "                global_id_counter += 1\n",
    "\n",
    "    # 4. ç”Ÿæˆ DataFrame å¹¶ä¿å­˜\n",
    "    if not new_rows:\n",
    "        print(\"âŒ æœªç”Ÿæˆä»»ä½•æ•°æ®ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–æ–‡ä»¶ååŒ¹é…è§„åˆ™ã€‚\")\n",
    "        return\n",
    "\n",
    "    df_final = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # è°ƒæ•´åˆ—é¡ºåº (æŠŠ parent_filename æ”¾åœ¨åé¢ï¼ŒID æ”¾åœ¨æœ€å‰)\n",
    "    cols = list(df_final.columns)\n",
    "    if 'parent_filename' in cols:\n",
    "        cols.remove('parent_filename')\n",
    "        cols.append('parent_filename') # ç§»åˆ°æœ€å\n",
    "    \n",
    "    # ç¡®ä¿ ID åœ¨ç¬¬ä¸€åˆ—\n",
    "    if 'ID' in cols:\n",
    "        cols.remove('ID')\n",
    "        cols.insert(0, 'ID')\n",
    "        \n",
    "    df_final = df_final[cols]\n",
    "\n",
    "    # ä¿å­˜\n",
    "    df_final.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"âœ… æ–°æ ‡æ³¨æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼\")\n",
    "    print(f\"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {OUTPUT_CSV_PATH}\")\n",
    "    print(f\"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d641032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹å…ƒæ•°æ®åŠ è½½å®Œæˆï¼Œå…± 23001 æ¡ç´¢å¼•ã€‚\n",
      "ğŸš€ å¼€å§‹æ‰«æå¹¶ç”Ÿæˆæ–°æ ‡æ³¨...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "âœ… æ–°æ ‡æ³¨æ–‡ä»¶ç”ŸæˆæˆåŠŸï¼\n",
      "ğŸ“„ è¾“å‡ºæ–‡ä»¶: X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\annotation\\DeepShip_No_Overlap_add_Environmental_Noise_Segmented_Metadata.csv\n",
      "ğŸ“Š æ€»æ ·æœ¬æ•°: 84236\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œ\n",
    "generate_segmented_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "321bc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°ç”Ÿæˆçš„æ ‡æ³¨æ–‡ä»¶\n",
    "CSV_PATH = r\"X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\annotation\\DeepShip_No_Overlap_add_Environmental_Noise_Segmented_Metadata.csv\"\n",
    "\n",
    "# é¢„å¤„ç†åçš„éŸ³é¢‘æ ¹ç›®å½•\n",
    "DATA_ROOT = Path(r\"X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\data_audio_rename_add_enviromental_noise_class_dataset_split_preprocessed\")\n",
    "\n",
    "# ç±»åˆ«æ˜ å°„\n",
    "ID_TO_FOLDER = {\n",
    "    0: 'Cargo',\n",
    "    1: 'Passengership',\n",
    "    2: 'Tanker',\n",
    "    3: 'Tug',\n",
    "    4: 'Background'\n",
    "}\n",
    "\n",
    "# ================= æ ¡éªŒé€»è¾‘ =================\n",
    "def verify_dataset_integrity():\n",
    "    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®é›†å®Œæ•´æ€§æ ¡éªŒ...\\n\")\n",
    "    \n",
    "    # --- æ­¥éª¤ 1: è¯»å– CSV ---\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        print(f\"âŒ è‡´å‘½é”™è¯¯: æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶ {CSV_PATH}\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"ğŸ“„ CSV è®°å½•æ€»æ•°: {len(df)}\")\n",
    "    \n",
    "    # æ„å»º CSV ä¸­çš„æ–‡ä»¶é›†åˆ: set of (split, category_name, filename)\n",
    "    csv_index = set()\n",
    "    csv_counts = {} # ç”¨äºç»Ÿè®¡å„ç±»åˆ«çš„æ•°é‡\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        split = row['dataset_split']\n",
    "        class_id = row['class_id']\n",
    "        fname = row['new_filename']\n",
    "        \n",
    "        cat_name = ID_TO_FOLDER.get(class_id)\n",
    "        if not cat_name:\n",
    "            print(f\"âŒ CSV åŒ…å«æœªçŸ¥çš„ class_id: {class_id}\")\n",
    "            continue\n",
    "            \n",
    "        # è®°å½•å…³é”®ç‰¹å¾å…ƒç»„\n",
    "        key = (split, cat_name, fname)\n",
    "        csv_index.add(key)\n",
    "        \n",
    "        # ç»Ÿè®¡\n",
    "        if cat_name not in csv_counts: csv_counts[cat_name] = 0\n",
    "        csv_counts[cat_name] += 1\n",
    "\n",
    "    # --- æ­¥éª¤ 2: æ‰«æç¡¬ç›˜ ---\n",
    "    if not DATA_ROOT.exists():\n",
    "        print(f\"âŒ è‡´å‘½é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨ {DATA_ROOT}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“‚ æ­£åœ¨æ‰«æç¡¬ç›˜ç›®å½•: {DATA_ROOT} ...\")\n",
    "    disk_index = set()\n",
    "    disk_counts = {}\n",
    "    \n",
    "    # éå† train å’Œ test\n",
    "    for split in ['train', 'test']:\n",
    "        split_dir = DATA_ROOT / split\n",
    "        if not split_dir.exists(): continue\n",
    "        \n",
    "        # éå†ç±»åˆ«\n",
    "        for cat_dir in split_dir.iterdir():\n",
    "            if not cat_dir.is_dir(): continue\n",
    "            cat_name = cat_dir.name\n",
    "            \n",
    "            # æ‰«ææ–‡ä»¶\n",
    "            files = list(cat_dir.glob(\"*.wav\"))\n",
    "            \n",
    "            # ç»Ÿè®¡\n",
    "            if cat_name not in disk_counts: disk_counts[cat_name] = 0\n",
    "            disk_counts[cat_name] += len(files)\n",
    "            \n",
    "            for f in files:\n",
    "                # è®°å½•å…³é”®ç‰¹å¾å…ƒç»„\n",
    "                key = (split, cat_name, f.name)\n",
    "                disk_index.add(key)\n",
    "\n",
    "    print(f\"ğŸ’¾ ç¡¬ç›˜æ–‡ä»¶æ€»æ•°: {len(disk_index)}\")\n",
    "\n",
    "    # --- æ­¥éª¤ 3: æ ¸å¿ƒå¯¹æ¯” ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ğŸ“Š æ ¡éªŒç»“æœæŠ¥å‘Š\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # æ£€æŸ¥ CSV ä¸­æœ‰ï¼Œä½†ç¡¬ç›˜æ²¡æœ‰çš„ (Missing Files)\n",
    "    missing_on_disk = csv_index - disk_index\n",
    "    if len(missing_on_disk) == 0:\n",
    "        print(\"âœ… [ä¸€è‡´] CSV ä¸­è®°å½•çš„æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ç¡¬ç›˜ä¸Šæ‰¾åˆ°äº†ã€‚\")\n",
    "    else:\n",
    "        print(f\"âŒ [ä¸ä¸€è‡´] ç¡¬ç›˜ç¼ºå¤±æ–‡ä»¶æ•°: {len(missing_on_disk)}\")\n",
    "        print(\"   å‰5ä¸ªç¼ºå¤±ç¤ºä¾‹:\")\n",
    "        for item in list(missing_on_disk)[:5]:\n",
    "            print(f\"   - é¢„æœŸä½ç½®: {item[0]}/{item[1]}/{item[2]}\")\n",
    "\n",
    "    # æ£€æŸ¥ç¡¬ç›˜ä¸Šæœ‰ï¼Œä½† CSV æ²¡è®°å½•çš„ (Orphan Files)\n",
    "    orphan_files = disk_index - csv_index\n",
    "    if len(orphan_files) == 0:\n",
    "        print(\"âœ… [ä¸€è‡´] ç¡¬ç›˜ä¸Šçš„æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ CSV ä¸­æœ‰è®°å½•ã€‚\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ [è­¦å‘Š] å‘ç°æœªæ ‡æ³¨çš„å¤šä½™æ–‡ä»¶æ•°: {len(orphan_files)}\")\n",
    "        print(\"   è¿™äº›æ–‡ä»¶å­˜åœ¨äºæ–‡ä»¶å¤¹ä¸­ï¼Œä½†ä¸åœ¨ CSV é‡Œï¼ˆå¯èƒ½éœ€è¦åˆ é™¤æˆ–é‡æ–°ç”ŸæˆCSVï¼‰ã€‚\")\n",
    "        print(\"   å‰5ä¸ªå¤šä½™ç¤ºä¾‹:\")\n",
    "        for item in list(orphan_files)[:5]:\n",
    "            print(f\"   - å®é™…ä½ç½®: {item[0]}/{item[1]}/{item[2]}\")\n",
    "\n",
    "    # æ•°é‡ç»Ÿè®¡å¯¹æ¯”\n",
    "    print(\"\\nğŸ“ˆ ç±»åˆ«æ•°é‡å¯¹æ¯” (CSV vs Disk):\")\n",
    "    all_cats = set(csv_counts.keys()) | set(disk_counts.keys())\n",
    "    for cat in sorted(list(all_cats)):\n",
    "        c_count = csv_counts.get(cat, 0)\n",
    "        d_count = disk_counts.get(cat, 0)\n",
    "        status = \"âœ… OK\" if c_count == d_count else \"âŒ Mismatch\"\n",
    "        print(f\"   - {cat:<15}: CSV={c_count:<6} | Disk={d_count:<6} | {status}\")\n",
    "\n",
    "    # --- æœ€ç»ˆç»“è®º ---\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    if len(missing_on_disk) == 0 and len(orphan_files) == 0:\n",
    "        print(\"ğŸ‰ éªŒè¯é€šè¿‡ï¼æ•°æ®é›†ä¸æ ‡æ³¨æ–‡ä»¶å®Œç¾åŒ¹é…ã€‚\")\n",
    "        print(\"   æ‚¨å¯ä»¥æ”¾å¿ƒå¼€å§‹è®­ç»ƒäº†ã€‚\")\n",
    "    else:\n",
    "        print(\"ğŸš« éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯ã€‚\")\n",
    "        print(\"   å»ºè®®ï¼šé‡æ–°è¿è¡Œç”Ÿæˆæ ‡æ³¨æ–‡ä»¶çš„è„šæœ¬ï¼Œæˆ–è€…æ£€æŸ¥é¢„å¤„ç†è¿‡ç¨‹ä¸­æ˜¯å¦æœ‰æ–‡ä»¶å†™å…¥å¤±è´¥ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c98ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®é›†å®Œæ•´æ€§æ ¡éªŒ...\n",
      "\n",
      "ğŸ“„ CSV è®°å½•æ€»æ•°: 84236\n",
      "ğŸ“‚ æ­£åœ¨æ‰«æç¡¬ç›˜ç›®å½•: X:\\æ•°æ®é›†\\DeepShip\\data_preprocessing\\data_audio_rename_add_enviromental_noise_class_dataset_split_preprocessed ...\n",
      "ğŸ’¾ ç¡¬ç›˜æ–‡ä»¶æ€»æ•°: 84236\n",
      "\n",
      "========================================\n",
      "ğŸ“Š æ ¡éªŒç»“æœæŠ¥å‘Š\n",
      "========================================\n",
      "âœ… [ä¸€è‡´] CSV ä¸­è®°å½•çš„æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ç¡¬ç›˜ä¸Šæ‰¾åˆ°äº†ã€‚\n",
      "âœ… [ä¸€è‡´] ç¡¬ç›˜ä¸Šçš„æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨ CSV ä¸­æœ‰è®°å½•ã€‚\n",
      "\n",
      "ğŸ“ˆ ç±»åˆ«æ•°é‡å¯¹æ¯” (CSV vs Disk):\n",
      "   - Background     : CSV=27768  | Disk=27768  | âœ… OK\n",
      "   - Cargo          : CSV=12801  | Disk=12801  | âœ… OK\n",
      "   - Passengership  : CSV=15410  | Disk=15410  | âœ… OK\n",
      "   - Tanker         : CSV=14762  | Disk=14762  | âœ… OK\n",
      "   - Tug            : CSV=13495  | Disk=13495  | âœ… OK\n",
      "\n",
      "----------------------------------------\n",
      "ğŸ‰ éªŒè¯é€šè¿‡ï¼æ•°æ®é›†ä¸æ ‡æ³¨æ–‡ä»¶å®Œç¾åŒ¹é…ã€‚\n",
      "   æ‚¨å¯ä»¥æ”¾å¿ƒå¼€å§‹è®­ç»ƒäº†ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œ\n",
    "verify_dataset_integrity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
